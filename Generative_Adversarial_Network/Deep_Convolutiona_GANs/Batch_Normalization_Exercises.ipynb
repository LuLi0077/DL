{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Batch Normalization – Practice"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Batch normalization is most useful when building deep neural networks. To demonstrate this, we'll create a convolutional neural network with 20 convolutional layers, followed by a fully connected layer. We'll use it to classify handwritten digits in the MNIST dataset. (This is **not** a good network for classfying MNIST digits. You could create a _much_ simpler network and get _better_ results.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook includes two versions of the network that you can edit. The first uses higher level functions from the `tf.layers` package. The second is the same network, but uses only lower level functions in the `tf.nn` package.\n",
    "\n",
    "1. [Batch Normalization with `tf.layers.batch_normalization`](#example_1)\n",
    "2. [Batch Normalization with `tf.nn.batch_normalization`](#example_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following cell loads TensorFlow, downloads the MNIST dataset if necessary, and loads it into an object named `mnist`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully downloaded train-images-idx3-ubyte.gz 9912422 bytes.\n",
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "Successfully downloaded train-labels-idx1-ubyte.gz 28881 bytes.\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Successfully downloaded t10k-images-idx3-ubyte.gz 1648877 bytes.\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Successfully downloaded t10k-labels-idx1-ubyte.gz 4542 bytes.\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets(\"MNIST_data/\", one_hot=True, reshape=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Batch Normalization using `tf.layers.batch_normalization`<a id=\"example_1\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll use the following function to create fully connected layers in our network. We'll create them with the specified number of neurons and a ReLU activation function.\n",
    "\n",
    "This version of the function does not include batch normalization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def fully_connected(prev_layer, num_units):\n",
    "    \"\"\"\n",
    "    Create a fully connectd layer with the given layer as input and the given number of neurons.\n",
    "    \n",
    "    :param prev_layer: Tensor\n",
    "        The Tensor that acts as input into this layer\n",
    "    :param num_units: int\n",
    "        The size of the layer. That is, the number of units, nodes, or neurons.\n",
    "    :returns Tensor\n",
    "        A new fully connected layer\n",
    "    \"\"\"\n",
    "    layer = tf.layers.dense(prev_layer, num_units, activation=tf.nn.relu)\n",
    "    return layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll use the following function to create convolutional layers in our network. They are very basic: we're always using a 3x3 kernel, ReLU activation functions, strides of 1x1 on layers with odd depths, and strides of 2x2 on layers with even depths. We aren't bothering with pooling layers at all in this network.\n",
    "\n",
    "This version of the function does not include batch normalization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def conv_layer(prev_layer, layer_depth):\n",
    "    \"\"\"\n",
    "    Create a convolutional layer with the given layer as input.\n",
    "    \n",
    "    :param prev_layer: Tensor\n",
    "        The Tensor that acts as input into this layer\n",
    "    :param layer_depth: int\n",
    "        We'll set the strides and number of feature maps based on the layer's depth in the network.\n",
    "        This is *not* a good way to make a CNN, but it helps us create this example with very little code.\n",
    "    :returns Tensor\n",
    "        A new convolutional layer\n",
    "    \"\"\"\n",
    "    strides = 2 if layer_depth % 3 == 0 else 1\n",
    "    conv_layer = tf.layers.conv2d(prev_layer, layer_depth*4, 3, strides, 'same', activation=tf.nn.relu)\n",
    "    return conv_layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This cell builds the network **without** batch normalization, then trains it on the MNIST dataset. It displays loss and accuracy data periodically while training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch:  0: Validation loss: 0.69126, Validation accuracy: 0.11260\n",
      "Batch: 25: Training loss: 0.42254, Training accuracy: 0.09375\n",
      "Batch: 50: Training loss: 0.32589, Training accuracy: 0.07812\n",
      "Batch: 75: Training loss: 0.32734, Training accuracy: 0.06250\n",
      "Batch: 100: Validation loss: 0.32492, Validation accuracy: 0.11260\n",
      "Batch: 125: Training loss: 0.32684, Training accuracy: 0.07812\n",
      "Batch: 150: Training loss: 0.32619, Training accuracy: 0.09375\n",
      "Batch: 175: Training loss: 0.32238, Training accuracy: 0.21875\n",
      "Batch: 200: Validation loss: 0.32555, Validation accuracy: 0.09860\n",
      "Batch: 225: Training loss: 0.32437, Training accuracy: 0.15625\n",
      "Batch: 250: Training loss: 0.32619, Training accuracy: 0.07812\n",
      "Batch: 275: Training loss: 0.32294, Training accuracy: 0.12500\n",
      "Batch: 300: Validation loss: 0.32544, Validation accuracy: 0.11260\n",
      "Batch: 325: Training loss: 0.32258, Training accuracy: 0.15625\n",
      "Batch: 350: Training loss: 0.32334, Training accuracy: 0.12500\n",
      "Batch: 375: Training loss: 0.32592, Training accuracy: 0.09375\n",
      "Batch: 400: Validation loss: 0.32493, Validation accuracy: 0.11260\n",
      "Batch: 425: Training loss: 0.32542, Training accuracy: 0.09375\n",
      "Batch: 450: Training loss: 0.32651, Training accuracy: 0.10938\n",
      "Batch: 475: Training loss: 0.32504, Training accuracy: 0.12500\n",
      "Batch: 500: Validation loss: 0.32511, Validation accuracy: 0.10020\n",
      "Batch: 525: Training loss: 0.32747, Training accuracy: 0.12500\n",
      "Batch: 550: Training loss: 0.32393, Training accuracy: 0.09375\n",
      "Batch: 575: Training loss: 0.32277, Training accuracy: 0.20312\n",
      "Batch: 600: Validation loss: 0.32525, Validation accuracy: 0.11260\n",
      "Batch: 625: Training loss: 0.32514, Training accuracy: 0.14062\n",
      "Batch: 650: Training loss: 0.32804, Training accuracy: 0.07812\n",
      "Batch: 675: Training loss: 0.32566, Training accuracy: 0.12500\n",
      "Batch: 700: Validation loss: 0.32513, Validation accuracy: 0.09860\n",
      "Batch: 725: Training loss: 0.32491, Training accuracy: 0.09375\n",
      "Batch: 750: Training loss: 0.32535, Training accuracy: 0.14062\n",
      "Batch: 775: Training loss: 0.32563, Training accuracy: 0.14062\n",
      "Final validation accuracy: 0.11000\n",
      "Final test accuracy: 0.10280\n",
      "Accuracy on 100 samples: 0.15\n"
     ]
    }
   ],
   "source": [
    "def train(num_batches, batch_size, learning_rate):\n",
    "    # Build placeholders for the input samples and labels \n",
    "    inputs = tf.placeholder(tf.float32, [None, 28, 28, 1])\n",
    "    labels = tf.placeholder(tf.float32, [None, 10])\n",
    "    \n",
    "    # Feed the inputs into a series of 20 convolutional layers \n",
    "    layer = inputs\n",
    "    for layer_i in range(1, 20):\n",
    "        layer = conv_layer(layer, layer_i)\n",
    "\n",
    "    # Flatten the output from the convolutional layers \n",
    "    orig_shape = layer.get_shape().as_list()\n",
    "    layer = tf.reshape(layer, shape=[-1, orig_shape[1] * orig_shape[2] * orig_shape[3]])\n",
    "\n",
    "    # Add one fully connected layer\n",
    "    layer = fully_connected(layer, 100)\n",
    "\n",
    "    # Create the output layer with 1 node for each \n",
    "    logits = tf.layers.dense(layer, 10)\n",
    "    \n",
    "    # Define loss and training operations\n",
    "    model_loss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=logits, labels=labels))\n",
    "    train_opt = tf.train.AdamOptimizer(learning_rate).minimize(model_loss)\n",
    "    \n",
    "    # Create operations to test accuracy\n",
    "    correct_prediction = tf.equal(tf.argmax(logits,1), tf.argmax(labels,1))\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "    \n",
    "    # Train and test the network\n",
    "    with tf.Session() as sess:\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        for batch_i in range(num_batches):\n",
    "            batch_xs, batch_ys = mnist.train.next_batch(batch_size)\n",
    "\n",
    "            # train this batch\n",
    "            sess.run(train_opt, {inputs: batch_xs, labels: batch_ys})\n",
    "            \n",
    "            # Periodically check the validation or training loss and accuracy\n",
    "            if batch_i % 100 == 0:\n",
    "                loss, acc = sess.run([model_loss, accuracy], {inputs: mnist.validation.images,\n",
    "                                                              labels: mnist.validation.labels})\n",
    "                print('Batch: {:>2}: Validation loss: {:>3.5f}, Validation accuracy: {:>3.5f}'.format(batch_i, loss, acc))\n",
    "            elif batch_i % 25 == 0:\n",
    "                loss, acc = sess.run([model_loss, accuracy], {inputs: batch_xs, labels: batch_ys})\n",
    "                print('Batch: {:>2}: Training loss: {:>3.5f}, Training accuracy: {:>3.5f}'.format(batch_i, loss, acc))\n",
    "\n",
    "        # At the end, score the final accuracy for both the validation and test sets\n",
    "        acc = sess.run(accuracy, {inputs: mnist.validation.images,\n",
    "                                  labels: mnist.validation.labels})\n",
    "        print('Final validation accuracy: {:>3.5f}'.format(acc))\n",
    "        acc = sess.run(accuracy, {inputs: mnist.test.images,\n",
    "                                  labels: mnist.test.labels})\n",
    "        print('Final test accuracy: {:>3.5f}'.format(acc))\n",
    "        \n",
    "        # Score the first 100 test images individually. This won't work if batch normalization isn't implemented correctly.\n",
    "        correct = 0\n",
    "        for i in range(100):\n",
    "            correct += sess.run(accuracy,feed_dict={inputs: [mnist.test.images[i]],\n",
    "                                                    labels: [mnist.test.labels[i]]})\n",
    "\n",
    "        print(\"Accuracy on 100 samples:\", correct/100)\n",
    "\n",
    "\n",
    "num_batches = 800\n",
    "batch_size = 64\n",
    "learning_rate = 0.002\n",
    "\n",
    "tf.reset_default_graph()\n",
    "with tf.Graph().as_default():\n",
    "    train(num_batches, batch_size, learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With this many layers, it's going to take a lot of iterations for this network to learn. By the time you're done training these 800 batches, your final test and validation accuracies probably won't be much better than 10%. (It will be different each time, but will most likely be less than 15%.)\n",
    "\n",
    "Using batch normalization, you'll be able to train this same network to over 90% in that same number of batches."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Add batch normalization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To add batch normalization to the layers created by `fully_connected`:\n",
    "1. Added the `is_training` parameter to the function signature so we can pass that information to the batch normalization layer.\n",
    "2. Removed the bias and activation function from the `dense` layer.\n",
    "3. Used `tf.layers.batch_normalization` to normalize the layer's output. We pass `is_training` to this layer to ensure the network updates its population statistics appropriately.\n",
    "4. Passed the normalized values into a ReLU activation function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def fully_connected(prev_layer, num_units, is_training):\n",
    "    \"\"\"\n",
    "    Create a fully connectd layer with the given layer as input and the given number of neurons.\n",
    "    \n",
    "    :param prev_layer: Tensor\n",
    "        The Tensor that acts as input into this layer\n",
    "    :param num_units: int\n",
    "        The size of the layer. That is, the number of units, nodes, or neurons.\n",
    "    :returns Tensor\n",
    "        A new fully connected layer\n",
    "    \"\"\"\n",
    "    layer = tf.layers.dense(prev_layer, num_units, use_bias=False, activation=None)\n",
    "    layer = tf.layers.batch_normalization(layer, training=is_training)\n",
    "    layer = tf.nn.relu(layer)\n",
    "    return layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To add batch normalization to the layers created by `conv_layer`:\n",
    "1. Added the `is_training` parameter to the function signature so we can pass that information to the batch normalization layer.\n",
    "2. Removed the bias and activation function from the `conv2d` layer.\n",
    "3. Used `tf.layers.batch_normalization` to normalize the convolutional layer's output. We pass `is_training` to this layer to ensure the network updates its population statistics appropriately.\n",
    "4. Passed the normalized values into a ReLU activation function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def conv_layer(prev_layer, layer_depth, is_training):\n",
    "    \"\"\"\n",
    "    Create a convolutional layer with the given layer as input.\n",
    "    \n",
    "    :param prev_layer: Tensor\n",
    "        The Tensor that acts as input into this layer\n",
    "    :param layer_depth: int\n",
    "        We'll set the strides and number of feature maps based on the layer's depth in the network.\n",
    "        This is *not* a good way to make a CNN, but it helps us create this example with very little code.\n",
    "    :returns Tensor\n",
    "        A new convolutional layer\n",
    "    \"\"\"\n",
    "    strides = 2 if layer_depth % 3 == 0 else 1\n",
    "    conv_layer = tf.layers.conv2d(prev_layer, layer_depth*4, 3, strides, 'same', use_bias=False, activation=None)\n",
    "    conv_layer = tf.layers.batch_normalization(conv_layer, training=is_training)\n",
    "    conv_layer = tf.nn.relu(conv_layer)    \n",
    "    return conv_layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Batch normalization is still a new enough idea that researchers are still discovering how best to use it. In general, people seem to agree to remove the layer's bias (because the batch normalization already has terms for scaling and shifting) and add batch normalization _before_ the layer's non-linear activation function. However, for some networks it will work well in other ways, too. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Alternate solution that uses bias in the convolutional layer but still adds batch normalization before the ReLU activation function.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def conv_layer(prev_layer, layer_num, is_training):\n",
    "    strides = 2 if layer_num % 3 == 0 else 1\n",
    "    conv_layer = tf.layers.conv2d(prev_layer, layer_num*4, 3, strides, 'same', use_bias=True, activation=None)\n",
    "    conv_layer = tf.layers.batch_normalization(conv_layer, training=is_training)\n",
    "    conv_layer = tf.nn.relu(conv_layer)\n",
    "    return conv_layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Alternate solution that uses a bias and ReLU activation function _before_ batch normalization.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def conv_layer(prev_layer, layer_num, is_training):\n",
    "    strides = 2 if layer_num % 3 == 0 else 1\n",
    "    conv_layer = tf.layers.conv2d(prev_layer, layer_num*4, 3, strides, 'same', use_bias=True, activation=tf.nn.relu)\n",
    "    conv_layer = tf.layers.batch_normalization(conv_layer, training=is_training)\n",
    "    return conv_layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Alternate solution that uses a ReLU activation function _before_ normalization, but no bias.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def conv_layer(prev_layer, layer_num, is_training):\n",
    "    strides = 2 if layer_num % 3 == 0 else 1\n",
    "    conv_layer = tf.layers.conv2d(prev_layer, layer_num*4, 3, strides, 'same', use_bias=False, activation=tf.nn.relu)\n",
    "    conv_layer = tf.layers.batch_normalization(conv_layer, training=is_training)\n",
    "    return conv_layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To modify `train`:\n",
    "1. Added `is_training`, a placeholder to store a boolean value indicating whether or not the network is training.\n",
    "2. Passed `is_training` to the `conv_layer` and `fully_connected` functions.\n",
    "3. Each time we call `run` on the session, we added to `feed_dict` the appropriate value for `is_training`.\n",
    "4. Moved the creation of `train_opt` inside a `with tf.control_dependencies...` statement. This is necessary to get the normalization layers created with `tf.layers.batch_normalization` to update their population statistics, which we need when performing inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch:  0: Validation loss: 0.69100, Validation accuracy: 0.10020\n",
      "Batch: 25: Training loss: 0.57199, Training accuracy: 0.09375\n",
      "Batch: 50: Training loss: 0.46293, Training accuracy: 0.12500\n",
      "Batch: 75: Training loss: 0.39592, Training accuracy: 0.06250\n",
      "Batch: 100: Validation loss: 0.35954, Validation accuracy: 0.09900\n",
      "Batch: 125: Training loss: 0.33863, Training accuracy: 0.14062\n",
      "Batch: 150: Training loss: 0.33589, Training accuracy: 0.20312\n",
      "Batch: 175: Training loss: 0.32061, Training accuracy: 0.21875\n",
      "Batch: 200: Validation loss: 0.33222, Validation accuracy: 0.23000\n",
      "Batch: 225: Training loss: 0.20129, Training accuracy: 0.53125\n",
      "Batch: 250: Training loss: 0.12262, Training accuracy: 0.76562\n",
      "Batch: 275: Training loss: 0.21407, Training accuracy: 0.67188\n",
      "Batch: 300: Validation loss: 0.06620, Validation accuracy: 0.89720\n",
      "Batch: 325: Training loss: 0.07852, Training accuracy: 0.85938\n",
      "Batch: 350: Training loss: 0.06407, Training accuracy: 0.89062\n",
      "Batch: 375: Training loss: 0.04262, Training accuracy: 0.93750\n",
      "Batch: 400: Validation loss: 0.13906, Validation accuracy: 0.79500\n",
      "Batch: 425: Training loss: 0.03365, Training accuracy: 0.95312\n",
      "Batch: 450: Training loss: 0.05388, Training accuracy: 0.92188\n",
      "Batch: 475: Training loss: 0.08360, Training accuracy: 0.90625\n",
      "Batch: 500: Validation loss: 0.03374, Validation accuracy: 0.95260\n",
      "Batch: 525: Training loss: 0.02259, Training accuracy: 0.95312\n",
      "Batch: 550: Training loss: 0.03766, Training accuracy: 0.92188\n",
      "Batch: 575: Training loss: 0.04701, Training accuracy: 0.95312\n",
      "Batch: 600: Validation loss: 0.02518, Validation accuracy: 0.96420\n",
      "Batch: 625: Training loss: 0.03664, Training accuracy: 0.93750\n",
      "Batch: 650: Training loss: 0.02418, Training accuracy: 0.96875\n",
      "Batch: 675: Training loss: 0.01250, Training accuracy: 0.96875\n",
      "Batch: 700: Validation loss: 0.04398, Validation accuracy: 0.93040\n",
      "Batch: 725: Training loss: 0.00586, Training accuracy: 1.00000\n",
      "Batch: 750: Training loss: 0.00668, Training accuracy: 0.98438\n",
      "Batch: 775: Training loss: 0.03607, Training accuracy: 0.95312\n",
      "Final validation accuracy: 0.92920\n",
      "Final test accuracy: 0.92720\n",
      "Accuracy on 100 samples: 0.93\n"
     ]
    }
   ],
   "source": [
    "def train(num_batches, batch_size, learning_rate):\n",
    "    # Build placeholders for the input samples and labels \n",
    "    inputs = tf.placeholder(tf.float32, [None, 28, 28, 1])\n",
    "    labels = tf.placeholder(tf.float32, [None, 10])\n",
    "\n",
    "    # Add placeholder to indicate whether or not we're training the model\n",
    "    is_training = tf.placeholder(tf.bool)\n",
    "    \n",
    "    # Feed the inputs into a series of 20 convolutional layers \n",
    "    layer = inputs\n",
    "    for layer_i in range(1, 20):\n",
    "        layer = conv_layer(layer, layer_i, is_training)\n",
    "\n",
    "    # Flatten the output from the convolutional layers \n",
    "    orig_shape = layer.get_shape().as_list()\n",
    "    layer = tf.reshape(layer, shape=[-1, orig_shape[1] * orig_shape[2] * orig_shape[3]])\n",
    "\n",
    "    # Add one fully connected layer\n",
    "    layer = fully_connected(layer, 100, is_training)\n",
    "\n",
    "    # Create the output layer with 1 node for each \n",
    "    logits = tf.layers.dense(layer, 10)\n",
    "    \n",
    "    # Define loss and training operations\n",
    "    model_loss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=logits, labels=labels))\n",
    "    \n",
    "    # Tell TensorFlow to update the population statistics while training\n",
    "    with tf.control_dependencies(tf.get_collection(tf.GraphKeys.UPDATE_OPS)):\n",
    "        train_opt = tf.train.AdamOptimizer(learning_rate).minimize(model_loss)\n",
    "        \n",
    "    # Create operations to test accuracy\n",
    "    correct_prediction = tf.equal(tf.argmax(logits,1), tf.argmax(labels,1))\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "    \n",
    "    # Train and test the network\n",
    "    with tf.Session() as sess:\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        for batch_i in range(num_batches):\n",
    "            batch_xs, batch_ys = mnist.train.next_batch(batch_size)\n",
    "\n",
    "            # train this batch\n",
    "            sess.run(train_opt, {inputs: batch_xs, labels: batch_ys, is_training: True})\n",
    "            \n",
    "            # Periodically check the validation or training loss and accuracy\n",
    "            if batch_i % 100 == 0:\n",
    "                loss, acc = sess.run([model_loss, accuracy], {inputs: mnist.validation.images,\n",
    "                                                              labels: mnist.validation.labels,\n",
    "                                                              is_training: False})\n",
    "                print('Batch: {:>2}: Validation loss: {:>3.5f}, Validation accuracy: {:>3.5f}'.format(batch_i, loss, acc))\n",
    "            elif batch_i % 25 == 0:\n",
    "                loss, acc = sess.run([model_loss, accuracy], {inputs: batch_xs, labels: batch_ys, is_training: False})\n",
    "                print('Batch: {:>2}: Training loss: {:>3.5f}, Training accuracy: {:>3.5f}'.format(batch_i, loss, acc))\n",
    "\n",
    "        # At the end, score the final accuracy for both the validation and test sets\n",
    "        acc = sess.run(accuracy, {inputs: mnist.validation.images,\n",
    "                                  labels: mnist.validation.labels,\n",
    "                                  is_training: False})\n",
    "        print('Final validation accuracy: {:>3.5f}'.format(acc))\n",
    "        acc = sess.run(accuracy, {inputs: mnist.test.images,\n",
    "                                  labels: mnist.test.labels,\n",
    "                                  is_training: False})\n",
    "        print('Final test accuracy: {:>3.5f}'.format(acc))\n",
    "        \n",
    "        # Score the first 100 test images individually. This won't work if batch normalization isn't implemented correctly.\n",
    "        correct = 0\n",
    "        for i in range(100):\n",
    "            correct += sess.run(accuracy,feed_dict={inputs: [mnist.test.images[i]],\n",
    "                                                    labels: [mnist.test.labels[i]],\n",
    "                                                    is_training: False})\n",
    "\n",
    "        print(\"Accuracy on 100 samples:\", correct/100)\n",
    "\n",
    "\n",
    "num_batches = 800\n",
    "batch_size = 64\n",
    "learning_rate = 0.002\n",
    "\n",
    "tf.reset_default_graph()\n",
    "with tf.Graph().as_default():\n",
    "    train(num_batches, batch_size, learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With batch normalization, you should now get an accuracy over 90%. Notice also the last line of the output: `Accuracy on 100 samples`. If this value is low while everything else looks good, that means you did not implement batch normalization correctly. Specifically, it means you either did not calculate the population mean and variance while training, or you are not using those values during inference.\n",
    "\n",
    "# Batch Normalization using `tf.nn.batch_normalization`<a id=\"example_2\"></a>\n",
    "\n",
    "Most of the time you will be able to use higher level functions exclusively, but sometimes you may want to work at a lower level. For example, if you ever want to implement a new feature – something new enough that TensorFlow does not already include a high-level implementation of it, like batch normalization in an LSTM – then you may need to know these sorts of things.\n",
    "\n",
    "This version of the network uses `tf.nn` for almost everything, and expects you to implement batch normalization using [`tf.nn.batch_normalization`](https://www.tensorflow.org/api_docs/python/tf/nn/batch_normalization).\n",
    "\n",
    "This implementation of `fully_connected` is much more involved than the one that uses `tf.layers`. However, if you went through the `Batch_Normalization_Lesson` notebook, things should look pretty familiar. To add batch normalization, we did the following:\n",
    "1. Added the `is_training` parameter to the function signature so we can pass that information to the batch normalization layer.\n",
    "2. Removed the bias and activation function from the `dense` layer.\n",
    "3. Added `gamma`, `beta`, `pop_mean`, and `pop_variance` variables.\n",
    "4. Used `tf.cond` to make handle training and inference differently.\n",
    "5. When training, we use `tf.nn.moments` to calculate the batch mean and variance. Then we update the population statistics and use `tf.nn.batch_normalization` to normalize the layer's output using the batch statistics. Notice the `with tf.control_dependencies...` statement - this is required to force TensorFlow to run the operations that update the population statistics.\n",
    "6. During inference (i.e. when not training), we use `tf.nn.batch_normalization` to normalize the layer's output using the population statistics we calculated during training.\n",
    "7. Passed the normalized values into a ReLU activation function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def fully_connected(prev_layer, num_units, is_training):\n",
    "    \"\"\"\n",
    "    Create a fully connectd layer with the given layer as input and the given number of neurons.\n",
    "    \n",
    "    :param prev_layer: Tensor\n",
    "        The Tensor that acts as input into this layer\n",
    "    :param num_units: int\n",
    "        The size of the layer. That is, the number of units, nodes, or neurons.\n",
    "    :returns Tensor\n",
    "        A new fully connected layer\n",
    "    \"\"\"\n",
    "    layer = tf.layers.dense(prev_layer, num_units, use_bias=False, activation=None)\n",
    "    #layer = tf.layers.batch_normalization(layer, training=is_training)\n",
    "    gamma = tf.Variable(tf.ones([num_units]))\n",
    "    beta = tf.Variable(tf.zeros([num_units]))\n",
    "    \n",
    "    pop_mean = tf.Variable(tf.zeros([num_units]), trainable=False)\n",
    "    pop_variance = tf.Variable(tf.ones([num_units]), trainable=False)\n",
    "    \n",
    "    epsilon = 1e-3\n",
    "    \n",
    "    def batch_norm_training():\n",
    "        batch_mean, batch_variance = tf.nn.moments(layer, [0])\n",
    "        decay = 0.99\n",
    "        train_mean = tf.assign(pop_mean, pop_mean*decay+batch_mean*(1-decay))\n",
    "        train_variance = tf.assign(pop_variance, pop_variance*decay+batch_variance*(1-decay))\n",
    "        \n",
    "        with tf.control_dependencies([train_mean, train_variance]):\n",
    "            return tf.nn.batch_normalization(layer, batch_mean, batch_variance, beta, gamma, epsilon)\n",
    "        \n",
    "    def batch_norm_inference():\n",
    "        return tf.nn.batch_normalization(layer, pop_mean, pop_variance, beta, gamma, epsilon)\n",
    "    \n",
    "    batch_normalized_output = tf.cond(is_training, batch_norm_training, batch_norm_inference)\n",
    "    return tf.nn.relu(batch_normalized_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The changes we made to `conv_layer` are _almost_ exactly the same as the ones we made to `fully_connected`. However, there is an important difference. Convolutional layers have multiple feature maps, and each feature map uses shared weights. So we need to make sure we calculate our batch and population statistics **per feature map** instead of per node in the layer.\n",
    "\n",
    "To accomplish this, we do **the same things** that we did in `fully_connected`, with two exceptions:\n",
    "1. The sizes of `gamma`, `beta`, `pop_mean` and `pop_variance` are set to the number of feature maps (output channels) instead of the number of output nodes.\n",
    "2. We change the parameters we pass to `tf.nn.moments` to make sure it calculates the mean and variance for the correct dimensions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def conv_layer(prev_layer, layer_depth, is_training):\n",
    "    \"\"\"\n",
    "    Create a convolutional layer with the given layer as input.\n",
    "    \n",
    "    :param prev_layer: Tensor\n",
    "        The Tensor that acts as input into this layer\n",
    "    :param layer_depth: int\n",
    "        We'll set the strides and number of feature maps based on the layer's depth in the network.\n",
    "        This is *not* a good way to make a CNN, but it helps us create this example with very little code.\n",
    "    :returns Tensor\n",
    "        A new convolutional layer\n",
    "    \"\"\"\n",
    "    strides = 2 if layer_depth % 3 == 0 else 1\n",
    "    in_channels = prev_layer.get_shape().as_list()[3]\n",
    "    out_channels = layer_depth*4\n",
    "    \n",
    "    weights = tf.Variable(tf.truncated_normal([3, 3, in_channels, out_channels], stddev=0.05))\n",
    "    \n",
    "    conv_layer = tf.nn.conv2d(prev_layer, weights, strides=[1,strides, strides, 1], padding='SAME')\n",
    "    \n",
    "    gamma = tf.Variable(tf.ones([out_channels]))\n",
    "    beta = tf.Variable(tf.zeros([out_channels]))\n",
    "    \n",
    "    pop_mean = tf.Variable(tf.zeros([out_channels]), trainable=False)\n",
    "    pop_variance = tf.Variable(tf.ones([out_channels]), trainable=False)\n",
    "    \n",
    "    epsilon = 1e-3\n",
    "    \n",
    "    def batch_norm_training():\n",
    "        batch_mean, batch_variance = tf.nn.moments(conv_layer, [0, 1, 2], keep_dims=False)\n",
    "        decay = 0.99\n",
    "        train_mean = tf.assign(pop_mean, pop_mean*decay+batch_mean*(1-decay))\n",
    "        train_variance = tf.assign(pop_variance, pop_variance*decay+batch_variance*(1-decay))\n",
    "        \n",
    "        with tf.control_dependencies([train_mean, train_variance]):\n",
    "            return tf.nn.batch_normalization(conv_layer, batch_mean, batch_variance, beta, gamma, epsilon)\n",
    "        \n",
    "    def batch_norm_inference():\n",
    "        return tf.nn.batch_normalization(conv_layer, pop_mean, pop_variance, beta, gamma, epsilon)\n",
    "    \n",
    "    batch_normalized_output = tf.cond(is_training, batch_norm_training, batch_norm_inference)\n",
    "    return tf.nn.relu(batch_normalized_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To modify `train`:\n",
    "1. Added `is_training`, a placeholder to store a boolean value indicating whether or not the network is training.\n",
    "2. Each time we call `run` on the session, we added to `feed_dict` the appropriate value for `is_training`.\n",
    "3. We did **not** need to add the `with tf.control_dependencies...` statement that we added in the network that used `tf.layers.batch_normalization` because we handled updating the population statistics ourselves in `conv_layer` and `fully_connected`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch:  0: Validation loss: 0.69131, Validation accuracy: 0.09860\n",
      "Batch: 25: Training loss: 0.59587, Training accuracy: 0.06250\n",
      "Batch: 50: Training loss: 0.48779, Training accuracy: 0.14062\n",
      "Batch: 75: Training loss: 0.41358, Training accuracy: 0.12500\n",
      "Batch: 100: Validation loss: 0.37168, Validation accuracy: 0.09240\n",
      "Batch: 125: Training loss: 0.35051, Training accuracy: 0.09375\n",
      "Batch: 150: Training loss: 0.34839, Training accuracy: 0.09375\n",
      "Batch: 175: Training loss: 0.35825, Training accuracy: 0.14062\n",
      "Batch: 200: Validation loss: 0.47886, Validation accuracy: 0.11260\n",
      "Batch: 225: Training loss: 0.53295, Training accuracy: 0.04688\n",
      "Batch: 250: Training loss: 0.74321, Training accuracy: 0.07812\n",
      "Batch: 275: Training loss: 0.92164, Training accuracy: 0.09375\n",
      "Batch: 300: Validation loss: 0.89929, Validation accuracy: 0.11260\n",
      "Batch: 325: Training loss: 0.68584, Training accuracy: 0.17188\n",
      "Batch: 350: Training loss: 0.56644, Training accuracy: 0.18750\n",
      "Batch: 375: Training loss: 0.81147, Training accuracy: 0.10938\n",
      "Batch: 400: Validation loss: 1.08716, Validation accuracy: 0.11260\n",
      "Batch: 425: Training loss: 1.23249, Training accuracy: 0.12500\n",
      "Batch: 450: Training loss: 0.64449, Training accuracy: 0.21875\n",
      "Batch: 475: Training loss: 0.09802, Training accuracy: 0.82812\n",
      "Batch: 500: Validation loss: 0.11579, Validation accuracy: 0.82840\n",
      "Batch: 525: Training loss: 0.00883, Training accuracy: 0.98438\n",
      "Batch: 550: Training loss: 0.07171, Training accuracy: 0.87500\n",
      "Batch: 575: Training loss: 0.06769, Training accuracy: 0.90625\n",
      "Batch: 600: Validation loss: 0.03610, Validation accuracy: 0.95160\n",
      "Batch: 625: Training loss: 0.05175, Training accuracy: 0.93750\n",
      "Batch: 650: Training loss: 0.03366, Training accuracy: 0.95312\n",
      "Batch: 675: Training loss: 0.05836, Training accuracy: 0.93750\n",
      "Batch: 700: Validation loss: 0.02877, Validation accuracy: 0.96220\n",
      "Batch: 725: Training loss: 0.03477, Training accuracy: 0.93750\n",
      "Batch: 750: Training loss: 0.06335, Training accuracy: 0.90625\n",
      "Batch: 775: Training loss: 0.02720, Training accuracy: 0.95312\n",
      "Final validation accuracy: 0.95940\n",
      "Final test accuracy: 0.95540\n",
      "Accuracy on 100 samples: 0.96\n"
     ]
    }
   ],
   "source": [
    "def train(num_batches, batch_size, learning_rate):\n",
    "    # Build placeholders for the input samples and labels \n",
    "    inputs = tf.placeholder(tf.float32, [None, 28, 28, 1])\n",
    "    labels = tf.placeholder(tf.float32, [None, 10])\n",
    "    \n",
    "    # Add placeholder to indicate whether or not we're training the model\n",
    "    is_training = tf.placeholder(tf.bool)\n",
    "    \n",
    "    # Feed the inputs into a series of 20 convolutional layers \n",
    "    layer = inputs\n",
    "    for layer_i in range(1, 20):\n",
    "        layer = conv_layer(layer, layer_i, is_training)\n",
    "\n",
    "    # Flatten the output from the convolutional layers \n",
    "    orig_shape = layer.get_shape().as_list()\n",
    "    layer = tf.reshape(layer, shape=[-1, orig_shape[1] * orig_shape[2] * orig_shape[3]])\n",
    "\n",
    "    # Add one fully connected layer\n",
    "    layer = fully_connected(layer, 100, is_training)\n",
    "\n",
    "    # Create the output layer with 1 node for each \n",
    "    logits = tf.layers.dense(layer, 10)\n",
    "    \n",
    "    # Define loss and training operations\n",
    "    model_loss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=logits, labels=labels))\n",
    "    train_opt = tf.train.AdamOptimizer(learning_rate).minimize(model_loss)\n",
    "    \n",
    "    # Create operations to test accuracy\n",
    "    correct_prediction = tf.equal(tf.argmax(logits,1), tf.argmax(labels,1))\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "    \n",
    "    # Train and test the network\n",
    "    with tf.Session() as sess:\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        for batch_i in range(num_batches):\n",
    "            batch_xs, batch_ys = mnist.train.next_batch(batch_size)\n",
    "\n",
    "            # train this batch\n",
    "            sess.run(train_opt, {inputs: batch_xs, labels: batch_ys, is_training: True})\n",
    "            \n",
    "            # Periodically check the validation or training loss and accuracy\n",
    "            if batch_i % 100 == 0:\n",
    "                loss, acc = sess.run([model_loss, accuracy], {inputs: mnist.validation.images,\n",
    "                                                              labels: mnist.validation.labels,\n",
    "                                                              is_training: False})\n",
    "                print('Batch: {:>2}: Validation loss: {:>3.5f}, Validation accuracy: {:>3.5f}'.format(batch_i, loss, acc))\n",
    "            elif batch_i % 25 == 0:\n",
    "                loss, acc = sess.run([model_loss, accuracy], {inputs: batch_xs, labels: batch_ys, is_training: False})\n",
    "                print('Batch: {:>2}: Training loss: {:>3.5f}, Training accuracy: {:>3.5f}'.format(batch_i, loss, acc))\n",
    "\n",
    "        # At the end, score the final accuracy for both the validation and test sets\n",
    "        acc = sess.run(accuracy, {inputs: mnist.validation.images,\n",
    "                                  labels: mnist.validation.labels,\n",
    "                                  is_training: False})\n",
    "        print('Final validation accuracy: {:>3.5f}'.format(acc))\n",
    "        acc = sess.run(accuracy, {inputs: mnist.test.images,\n",
    "                                  labels: mnist.test.labels,\n",
    "                                  is_training: False})\n",
    "        print('Final test accuracy: {:>3.5f}'.format(acc))\n",
    "        \n",
    "        # Score the first 100 test images individually. This won't work if batch normalization isn't implemented correctly.\n",
    "        correct = 0\n",
    "        for i in range(100):\n",
    "            correct += sess.run(accuracy,feed_dict={inputs: [mnist.test.images[i]],\n",
    "                                                    labels: [mnist.test.labels[i]],\n",
    "                                                    is_training: False})\n",
    "\n",
    "        print(\"Accuracy on 100 samples:\", correct/100)\n",
    "\n",
    "\n",
    "num_batches = 800\n",
    "batch_size = 64\n",
    "learning_rate = 0.002\n",
    "\n",
    "tf.reset_default_graph()\n",
    "with tf.Graph().as_default():\n",
    "    train(num_batches, batch_size, learning_rate)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
